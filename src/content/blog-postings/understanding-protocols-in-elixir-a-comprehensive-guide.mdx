---
'@type': 'BlogPosting'
'@id': 'https://lifeiscontent.net/blog-postings/understanding-protocols-in-elixir-a-comprehensive-guide'
url: 'https://lifeiscontent.net/blog-postings/understanding-protocols-in-elixir-a-comprehensive-guide'
mainEntityOfPage:
  '@type': 'WebPage'
  '@id': 'https://lifeiscontent.net/blog-postings/understanding-protocols-in-elixir-a-comprehensive-guide'
isPartOf:
  '@type': 'Blog'
  '@id': 'https://lifeiscontent.net/blog-postings'
  name: 'lifeiscontent.net Blog'
headline: 'Understanding Protocols in Elixir: A Comprehensive Guide'
description: 'A purposeful walkthrough of how Elixir protocols turn polymorphism into a design practice that balances clarity, extensibility, and humane developer experience.'
abstract: 'Protocols help thoughtful builders encode agreements between data and behavior without inheritance; this guide shows how to put that philosophy to work.'
datePublished: '2024-11-10'
dateModified: '2025-11-30'
inLanguage: 'en-US'
author:
  - lifeiscontent
keywords:
  - elixir
  - protocols
  - fp
  - polymorphism
---

# Protocols in Elixir as Systems Contracts

Purposeful systems work because every piece knows its role. In Elixir we don’t reach for base classes; we write protocols, small agreements that let data tell us how it wants to behave. When you treat protocols as design artifacts, you get code that scales with clarity instead of conditionals.

This is a field note on using protocols the way I do on teams: as contracts between engineering, design, and product intent.

## The pressure cooker: one encoder to rule them all

Every API eventually needs to turn “whatever data we have” into JSON. Without protocols, you end up with a sprawling function that knows too much about every type in the system:

```elixir
defmodule JSONEncoder do
  def encode(data) when is_binary(data), do: "\"#{data}\""
  def encode(data) when is_integer(data), do: Integer.to_string(data)
  def encode(data) when is_boolean(data), do: Atom.to_string(data)
  def encode(data) when is_list(data), do: "[#{Enum.map_join(data, ",", &encode/1)}]"
  # More and more clauses for every possible type...
end
```

It technically works, but it drags you into three traps:

- **Monolithic ownership:** every new type forces you back into `JSONEncoder`.
- **Unfriendly to outsiders:** third-party structs can’t extend the encoder without forking.
- **Narrative leak:** the function has to understand every domain concept to do its job.

Protocols give us a calmer approach.

## Protocols: contracts, not conditionals

Define the conversation once, let each type answer for itself:

```elixir
# Step 1: Define the contract
defprotocol JSONEncodable do
  @doc "Converts a value to its JSON string representation"
  def to_json(value)
end

# Step 2: Implement for basic types
defimpl JSONEncodable, for: BitString do
  def to_json(string), do: "\"#{String.replace(string, "\"", "\\\"")}\""
end

defimpl JSONEncodable, for: Integer do
  def to_json(integer), do: Integer.to_string(integer)
end

defimpl JSONEncodable, for: Boolean do
  def to_json(true), do: "true"
  def to_json(false), do: "false"
end

defimpl JSONEncodable, for: List do
  def to_json(list) do
    items = Enum.map_join(list, ",", &JSONEncodable.to_json/1)
    "[#{items}]"
  end
end
```

Now the system feels less like a switch statement and more like a collaboration:

```elixir
iex> JSONEncodable.to_json("hello")
"\"hello\""

iex> JSONEncodable.to_json(42)
"42"

iex> JSONEncodable.to_json(true)
"true"

iex> JSONEncodable.to_json([1, "hello", false])
"[1,\"hello\",false]"
```

But the true payoff shows up when you add types you didn’t plan for:

```elixir
# In a completely different module, even a different library:
defimpl JSONEncodable, for: Date do
  def to_json(date) do
    date
    |> Date.to_iso8601()
    |> JSONEncodable.to_json()
  end
end

defimpl JSONEncodable, for: DateTime do
  def to_json(datetime) do
    datetime
    |> DateTime.to_iso8601()
    |> JSONEncodable.to_json()
  end
end

defimpl JSONEncodable, for: Atom do
  def to_json(nil), do: "null"
  def to_json(atom) do
    atom
    |> Atom.to_string()
    |> JSONEncodable.to_json()
  end
end
```

Notice the pattern: convert the data to something already understood, delegate, move on. That’s systems thinking in miniature.

```elixir
iex> JSONEncodable.to_json(~D[2024-11-10])
"\"2024-11-10\""
```

No edits to the original module. No forks. Just extension.

## Working with Maps: The Foundation for Structs

To make our protocol truly useful, we need to handle maps, since that's what most complex data becomes:

```elixir
defimpl JSONEncodable, for: Map do
  def to_json(map) when map == %{}, do: "{}"
  def to_json(map) do
    pairs =
      map
      |> Enum.map(fn {key, value} ->
        key_json = JSONEncodable.to_json(to_string(key))
        value_json = JSONEncodable.to_json(value)
        "#{key_json}:#{value_json}"
      end)
      |> Enum.join(",")

    "{#{pairs}}"
  end
end
```

Now encoding nested data reads like a conversation instead of a chore:

```elixir
iex> data = %{"name" => "Alice", "age" => 30, "active" => true}
iex> JSONEncodable.to_json(data)
"{\"name\":\"Alice\",\"age\":30,\"active\":true}"
```

## Bring in the real domain stories

Every product story eventually needs encoding. Instead of shoving structs through generic helpers, let them describe themselves:

```elixir
defmodule User do
  defstruct [:id, :name, :email, :role, :created_at]
end

defmodule Article do
  defstruct [:id, :title, :content, :author_id, :published_at, :tags]
end
```

Now let's implement JSON encoding for these domain types:

```elixir
defimpl JSONEncodable, for: User do
  def to_json(%User{id: id, name: name, email: email, role: role}) do
    %{
      id: id,
      name: name,
      email: email,
      role: role
    }
    |> JSONEncodable.to_json()
  end
end

defimpl JSONEncodable, for: Article do
  def to_json(%Article{id: id, title: title, author_id: author_id, published_at: published_at, tags: tags}) do
    %{
      id: id,
      title: title,
      author_id: author_id,
      published_at: published_at,
      tags: tags
    }
    |> JSONEncodable.to_json()
  end
end
```

The deliberate move here: curate the fields you actually want to expose, convert to a map, and lean on the Map implementation. Composition > special cases.

Putting it to work:

```elixir
iex> user = %User{id: 1, name: "Alice", email: "alice@example.com", role: "admin"}
iex> JSONEncodable.to_json(user)
"{\"id\":1,\"name\":\"Alice\",\"email\":\"alice@example.com\",\"role\":\"admin\"}"

iex> article = %Article{id: 1, title: "Protocols Guide", author_id: 1, published_at: nil, tags: ["elixir", "tutorial"]}
iex> JSONEncodable.to_json(article)
"{\"id\":1,\"title\":\"Protocols Guide\",\"author_id\":1,\"published_at\":null,\"tags\":[\"elixir\",\"tutorial\"]}"
```

## Another canvas: validation as ritual

Protocols aren’t just for serialization. I treat them as agreements for any cross-cutting behavior, including validation. Same idea: define the contract, let each type own its rules.

Start with the protocol:

```elixir
defprotocol Validatable do
  @doc "Returns {:ok, value} if valid, {:error, errors} if invalid"
  def validate(value)
end

defimpl Validatable, for: User do
  def validate(%User{name: name, email: email} = user) do
    []
    |> validate_name(name)
    |> validate_email(email)
    |> case do
      [] -> {:ok, user}
      errors -> {:error, Enum.reverse(errors)}
    end
  end

  defp validate_name(errors, name) do
    if valid_name?(name), do: errors, else: [{:name, "Name must be 2-50 characters"} | errors]
  end

  defp validate_email(errors, email) do
    if valid_email?(email), do: errors, else: [{:email, "Email must be valid"} | errors]
  end

  defp valid_name?(name) when is_binary(name) do
    String.length(name) >= 2 and String.length(name) <= 50
  end
  defp valid_name?(_), do: false

  defp valid_email?(email) when is_binary(email) do
    String.contains?(email, "@") and String.length(email) > 5
  end
  defp valid_email?(_), do: false
end

defimpl Validatable, for: Article do
  def validate(%Article{title: title, content: content} = article) do
    []
    |> validate_title(title)
    |> validate_content(content)
    |> case do
      [] -> {:ok, article}
      errors -> {:error, Enum.reverse(errors)}
    end
  end

  defp validate_title(errors, title) do
    if valid_title?(title), do: errors, else: [{:title, "Title must be 5-200 characters"} | errors]
  end

  defp validate_content(errors, content) do
    if valid_content?(content), do: errors, else: [{:content, "Content must be at least 50 characters"} | errors]
  end

  defp valid_title?(title) when is_binary(title) do
    len = String.length(title)
    len >= 5 and len <= 200
  end
  defp valid_title?(_), do: false

  defp valid_content?(content) when is_binary(content) do
    String.length(content) >= 50
  end
  defp valid_content?(_), do: false
end
```

Each validation implementation feels like a checklist. You accumulate errors with intent, keep them structured, and avoid sprinkling conditionals across controllers.

Using the protocol:

```elixir
iex> user = %User{name: "Alice", email: "alice@example.com", role: "admin"}
iex> Validatable.validate(user)
{:ok, %User{name: "Alice", email: "alice@example.com", role: "admin"}}

iex> bad_user = %User{name: "A", email: "bad-email"}
iex> Validatable.validate(bad_user)
{:error, [name: "Name must be 2-50 characters", email: "Email must be valid"]}

# Now you can easily check specific field errors:
iex> {:error, errors} = Validatable.validate(bad_user)
iex> Keyword.get(errors, :name)
"Name must be 2-50 characters"
iex> Keyword.has_key?(errors, :email)
true
```

Keyword lists make downstream consumers such as forms, APIs, and analytics tooling far happier:

- Field-specific access: `Keyword.get(errors, :email)`
- Conditional logic: `Keyword.has_key?(errors, :name)`
- UI wiring: map errors directly to inputs
- Analytics: count errors by field, not raw strings
- Partial retries: re-validate only what failed

For nested data, just keep composing:

```elixir
# Complex validation with nested errors
def validate_nested_data(data) do
  []
  |> validate_user_info(data.user)
  |> validate_preferences(data.preferences)
  |> validate_billing(data.billing)
  |> format_errors()
end

# Group errors by section
defp format_errors([]), do: {:ok, data}
defp format_errors(errors) do
  grouped = Keyword.group_by(errors, &elem(&1, 0), &elem(&1, 1))
  {:error, grouped}
end
```

Now you can stitch together complex payloads without sacrificing clarity:

```elixir
iex> complex_data = %{
...>   "user" => %User{id: 1, name: "Alice", email: "alice@example.com", role: "admin"},
...>   "metadata" => %{"timestamp" => ~U[2024-11-10 14:30:00Z], "version" => 1}
...> }
iex> JSONEncodable.to_json(complex_data)
# Returns complete JSON with proper nesting and type conversion
```

## Standing on the shoulders of the standard library

Elixir already models this philosophy:

- `String.Chars` powers interpolation.
- `Inspect` keeps debugging civilized.
- `Enumerable` lets any collection join the party.

Implement those for your own structs and suddenly the entire ecosystem understands your data.

## When not to spin up a protocol

Thoughtful builders know when to stop:

- If a transformation is purely internal, plain functions are clearer.
- If only one type ever needs the behavior, a protocol introduces ceremony you don’t need.
- If every type behaves identically, use a helper and call it a day.

## Design heuristics I keep handy

- **Solve a real tension.** Protocols should remove friction people already feel.
- **Keep the contract tiny.** One responsibility, well documented.
- **Name with intent.** Conversations read better than abbreviations.
- **Teach your future self.** Leave docstrings that explain why the protocol exists.

## Closing the loop

Protocols are the quiet handshake that keeps Elixir systems flexible. They let us mix engineering rigor with a designer’s empathy: every type voices what it needs, the system listens, and nobody edits a giant conditional at 2 a.m.

Find the monolith in your codebase that knows too much. Turn it into a protocol. Watch how the rest of the system exhales.
